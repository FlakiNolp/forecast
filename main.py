from __future__ import annotations
import abc
import asyncio
import json
import logging
import os
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Iterable, Any
import backoff
import httpx
import pandas as pd
from openai import AsyncClient
from tqdm.auto import tqdm

from expressions import regex_dict

from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor


# =========================
# –ü—É–±–ª–∏—á–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
# =========================

class BaseModel(ABC):
    @abstractmethod
    def fit(self, news_df: pd.DataFrame, candles_df: pd.DataFrame) -> None: ...
    @abstractmethod
    def predict(self) -> pd.DataFrame: ...


@dataclass
class ModelConfig:
    proxy: str
    api_base: str = "https://openrouter.ai/api/v1"
    api_key_env: str = "API_KEY"
    model_name: str = "openai/gpt-4o-mini"
    timeout_sec: float = 10
    max_retries: int = 5
    stall_timeout_sec: float = 15.0
    max_concurrency: int = 30
    # —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –Ω—É–∂–µ–Ω
    bar_pos_sentiment: int = 0
    bar_pos_tickers: int = 1
    prediction_length: int = 20



def create_model(cfg: ModelConfig) -> BaseModel:
    """
    –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—É–±–ª–∏—á–Ω–∞—è —Ñ–∞–±—Ä–∏–∫–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º BaseModel.
    –í—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∫–ª–∞—Å—Å—ã –∏ ‚Äú–±–∏–ª–¥–µ—Ä—ã‚Äù —Å–∫—Ä—ã—Ç—ã.
    """
    return PipelineBuilder(cfg).build()


# =========================
# –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ç–∏–ø—ã/—É—Ç–∏–ª–∏—Ç—ã
# =========================

class LLMQuotaError(Exception):
    """LLM –≤–µ—Ä–Ω—É–ª 402/403 ‚Äî –Ω–∞–¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å LLM-–≤–µ—Ç–∫—É."""


async def gather_with_progress(coros: Iterable[asyncio.Future], desc: str, position: int = 0):
    """–°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (as_completed) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫."""
    async def _wrap(idx: int, c):
        try:
            res = await c
            return idx, res, None
        except Exception as e:
            return idx, None, e

    tasks = [asyncio.create_task(_wrap(i, c)) for i, c in enumerate(coros)]
    results: list[Any] = [None] * len(tasks)

    with tqdm(
        total=len(tasks),
        desc=desc,
        position=position,
        leave=True,
        dynamic_ncols=True,
        miniters=1,
        mininterval=0.1,
    ) as bar:
        for fut in asyncio.as_completed(tasks):
            idx, res, err = await fut
            results[idx] = err if err is not None else res
            bar.update(1)

    return results



# =========================
# –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π LLM-—Å–µ—Ä–≤–∏—Å (—Å backoff)
# =========================

@dataclass
class LLMConfig:
    base_url: str
    api_key_env: str
    model_name: str
    proxy: str
    timeout: float
    max_retries: int


class LLMService:
    """–ï–¥–∏–Ω—ã–π LLM-—Å–µ—Ä–≤–∏—Å —Å –æ–±—â–∏–º —Å–µ–º–∞—Ñ–æ—Ä–æ–º –∏ backoff."""

    def __init__(self, cfg: LLMConfig, semaphore: asyncio.Semaphore):
        self.cfg = cfg
        self.semaphore = semaphore
        self._http = httpx.AsyncClient(timeout=httpx.Timeout(self.cfg.timeout), proxy=self.cfg.proxy)
        self._client = AsyncClient(
            base_url=self.cfg.base_url,
            api_key=os.getenv(self.cfg.api_key_env, ""),
            http_client=self._http,
        )

    async def aclose(self):
        await self._client.close()
        await self._http.aclose()

    @staticmethod
    def _giveup(exc: Exception) -> bool:
        status = getattr(getattr(exc, "response", None), "status_code", None) or getattr(exc, "status_code", None)
        if status in (402, 403):  # –∫–≤–æ—Ç—ã/–∑–∞–ø—Ä–µ—Ç
            return True
        if status == 429:  # –æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ —Ä–µ—Ç—Ä–∞–∏
            return False
        return status is not None and not (500 <= int(status) <= 599)

    @backoff.on_exception(
        backoff.expo,
        Exception,
        max_tries=lambda self, *a, **k: self.cfg.max_retries,
        jitter=backoff.full_jitter,
        giveup=lambda e: LLMService._giveup(e),
    )
    async def chat_json(self, system_prompt: str, response_format: dict, text: str) -> dict:
        async with self.semaphore:
            try:
                resp = await self._client.chat.completions.create(
                    model=self.cfg.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": text},
                    ],
                    temperature=0,
                    response_format=response_format,
                )
            except Exception as e:
                status = getattr(e, "status_code", None)
                if hasattr(e, "response") and getattr(e, "response", None) is not None:
                    try:
                        status = e.response.status_code
                    except Exception:
                        pass
                if status in (402, 403):
                    raise LLMQuotaError(str(e)) from e
                raise

        msg = resp.choices[0].message
        parsed = getattr(msg, "parsed", None)
        return parsed if parsed is not None else json.loads(msg.content)


# =========================
# –°–µ–Ω—Ç–∏–º–µ–Ω—Ç (LLM)
# =========================

class SentimentExtractor(abc.ABC):
    @abc.abstractmethod
    async def add_sentiment(self, news_df: pd.DataFrame, position: int) -> None: ...


class SentimentLLMExtractor(SentimentExtractor):
    SYSTEM_PROMPT = (
        "–¢—ã ‚Äî —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞, —Ä–∞–±–æ—Ç–∞—é—â–∏–π —Å —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏. "
        "–í–µ—Ä–Ω–∏ –∫–ª–∞—Å—Å 0 (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ), 1 (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ) –∏–ª–∏ 2 (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ). "
        "–¢–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥."
    )
    RESPONSE_FORMAT = {
        "type": "json_schema",
        "json_schema": {
            "name": "sentiment_extraction",
            "schema": {
                "type": "object",
                "additionalProperties": False,
                "properties": {"sentiment": {"type": "integer", "enum": [0, 1, 2]}},
                "required": ["sentiment"],
            },
            "strict": True,
        },
    }

    def __init__(self, llm: LLMService):
        self._llm = llm

    async def add_sentiment(self, news_df: pd.DataFrame, position: int) -> None:
        texts = news_df["publication"].tolist()
        coros = (self._llm.chat_json(self.SYSTEM_PROMPT, self.RESPONSE_FORMAT, t) for t in texts)
        results = await gather_with_progress(coros, desc="–°–µ–Ω—Ç–∏–º–µ–Ω—Ç", position=position)
        out: list[int | None] = []
        for r in results:
            out.append(None if isinstance(r, Exception) else r.get("sentiment"))
        news_df["sentiment"] = out


# =========================
# –¢–∏–∫–µ—Ä—ã: —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ + –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —Å as_completed
# =========================

class TickerStrategy(abc.ABC):
    @abc.abstractmethod
    async def extract_one(self, text: str) -> list[str]: ...


class RegexTickerStrategy(TickerStrategy):
    def __init__(self):
        self._cache: dict[str, list[re.Pattern]] = regex_dict

    async def extract_one(self, text: str) -> list[str]:
        text_str = str(text).lower()
        found_tickers = set()
        # 1) –ü–æ—Ä—è–¥–æ–∫: —Å–Ω–∞—á–∞–ª–∞ OZPH (–µ—Å–ª–∏ –µ—Å—Ç—å), –∑–∞—Ç–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        keys = (["OZPH"] if "OZPH" in regex_dict else []) + [
            k for k in regex_dict if k != "OZPH"
        ]

        # 2) –ü–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–∫–µ—Ä—É –ø—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—å –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for ticker in keys:
            patterns = regex_dict[ticker]
            has_match = False
            for pat in patterns:
                if re.search(pat, text_str):
                    has_match = True
                    break
            if has_match and ticker not in found_tickers:
                found_tickers.add(ticker)

        # 3) –ï—Å–ª–∏ OZPH –Ω–∞–π–¥–µ–Ω ‚Äî OZON –∏—Å–∫–ª—é—á–∞–µ–º
        if "OZPH" in found_tickers:
            found_tickers = [t for t in found_tickers if t != "OZON"]

        return list(found_tickers)


class TickerExtractor:
    def __init__(self, strategy: TickerStrategy):
        self._strategy = strategy

    async def add_tickers(
        self,
        news_df: pd.DataFrame,
        llm_bar_pos: int,
    ) -> None:
        texts = news_df["publication"].tolist()
        coros = [self._strategy.extract_one(text) for text in texts]
        results = await gather_with_progress(coros, desc="–¢–∏–∫–µ—Ä—ã (Regex)", position=llm_bar_pos)
        news_df["tickers"] = results


# =========================
# –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π Pipeline + –±–∏–ª–¥–µ—Ä
# =========================

class SmartModel(BaseModel):
    def __init__(
        self,
        sentiment: SentimentExtractor,
        tickers: TickerExtractor,
        cfg: ModelConfig,
    ):
        self._sent = sentiment
        self._tix = tickers
        self._cfg = cfg
        self._news: pd.DataFrame | None = None
        self._candles: pd.DataFrame | None = None

        self.predictor = TimeSeriesPredictor(
            prediction_length=cfg.prediction_length,
            target="close",
            freq='D',
            eval_metric="WQL"
        )

        self.data = None

    def __resolve_daily_sentiment(self, group: pd.DataFrame) -> str:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç –¥–ª—è –≥—Ä—É–ø–ø—ã (–æ–¥–∏–Ω —Ç–∏–∫–µ—Ä –≤ –æ–¥–∏–Ω –¥–µ–Ω—å)
        –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º:
          1) –º–æ–¥–∞;
          2) –µ—Å–ª–∏ –Ω–∏—á—å—è ‚Äî –≤—ã–±–∏—Ä–∞–µ–º –Ω–µ 'neutral';
          3) –µ—Å–ª–∏ —Å–Ω–æ–≤–∞ –Ω–∏—á—å—è ‚Äî —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —Å–∞–º–æ–π –ø–æ–∑–¥–Ω–µ–π –Ω–æ–≤–æ—Å—Ç–∏.
        """
        # —Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º (–∫–∞–∫ –µ—Å—Ç—å) –º–µ—Ç–∫–∞–º
        counts = group['sentiment'].value_counts()
        max_count = counts.max()
        top = counts[counts == max_count].index.tolist()

        if len(top) == 1:
            return top[0]

        # –Ω–∏—á—å—è: –ø–æ–ø—Ä–æ–±—É–µ–º –≤—ã–±—Ä–∞—Ç—å –Ω–µ-–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—É—é –º–µ—Ç–∫—É
        top_non_neutral = [s for s in top if str(s).strip().lower() != 'neutral']
        if len(top_non_neutral) == 1:
            return top_non_neutral[0]

        # –µ—Å–ª–∏ –≤—Å—ë –µ—â—ë –Ω–µ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ ‚Äî –±–µ—Ä–µ–º —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —Å–∞–º–æ–π –ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–æ–≤–æ—Å—Ç–∏ –¥–Ω—è
        last_row = group.sort_values('publish_date', kind='mergesort').iloc[-1]
        return last_row['sentiment']

    def fit(self, news_df: pd.DataFrame, candles_df: pd.DataFrame) -> None:
        # if set(candles_df.columns) != {'open', 'close', 'high', 'low', 'volume', 'begin', 'ticker'}:
        #     raise ValueError('cancles_df must contains {"open", "close", "high", "low", "volume", "begin", "ticker"}')
        # if set(news_df.columns) != {'publish_date', 'title', 'publication'}:
        #     raise ValueError('news_df must contains {"publication_date", "title", "publication"}')
        self._news = news_df.copy()
        self._candles = candles_df.copy()

        # –∂–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        # –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        async def process():
            await asyncio.gather(
                self._sent.add_sentiment(self._news, position=self._cfg.bar_pos_sentiment),
                self._tix.add_tickers(self._news, llm_bar_pos=self._cfg.bar_pos_tickers),
            )

        asyncio.run(process())

        exploded_tickers_df = (
            self._news.explode('tickers', ignore_index=True)  # –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω —Ç–∏–∫–µ—Ä
            .rename(columns={'tickers': 'ticker'})  # –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –≤ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)
        )

        exploded_tickers_df = exploded_tickers_df.dropna(subset=['ticker'])
        exploded_tickers_df['date'] = pd.to_datetime(exploded_tickers_df['publish_date']).dt.date

        aggregated_news_daily = (
            exploded_tickers_df.sort_values(['ticker', 'publish_date'], kind='mergesort')
            .groupby(['ticker', 'date'], as_index=False)
            .apply(lambda g: pd.Series({'sentiment_daily': self.__resolve_daily_sentiment(g)}))
        )
        aggregated_news_daily = aggregated_news_daily[['ticker', 'date', 'sentiment_daily']]
        aggregated_news_daily['date'] = pd.to_datetime(aggregated_news_daily['date'])

        candles_df = self._candles.copy()
        candles_df['date'] = pd.to_datetime(pd.to_datetime(candles_df['begin']).dt.date)
        candles_df = candles_df[['date', 'volume', 'close', 'ticker']]

        final = candles_df.merge(aggregated_news_daily, on=['ticker', 'date'], how='left')
        # final.to_csv('FINAL.csv', index=False)

        # TRAIN
        df = final.rename(columns={'date': 'timestamp'})
        data = TimeSeriesDataFrame.from_data_frame(
            df,
            id_column='ticker',
        )
        data = data.convert_frequency(freq="D")
        self.data = data
        data.to_csv('YEEEES.csv', index=False)

        self.predictor.fit(
            data,
            hyperparameters={
                "TemporalFusionTransformerModel": [{}]
            },
            enable_ensemble=False,
        )


    def predict(self) -> pd.DataFrame:
        predictions = self.predictor.predict(self.data)
        predictions = predictions.to_data_frame().reset_index()
        predictions['close'] = predictions['0.5']
        predictions['ticker'] = predictions['item_id']
        predictions = predictions.drop(columns=['item_id', 'mean', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9'])
        predictions['begin'] = predictions['timestamp']
        data = self.data.groupby(['item_id']).tail(1).rename(columns={'item_id': 'ticker'})
        return pd.concat([data, predictions], ignore_index=True).sort_values([
            'ticker', 'begin'
        ]).reset_index(drop=True)


class PipelineBuilder:
    def __init__(self, cfg: ModelConfig):
        self._cfg = cfg

    def build(self) -> BaseModel:
        # —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —á–µ—Ä–µ–∑ LLM (–æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å)
        semaphore = asyncio.Semaphore(self._cfg.max_concurrency)
        llm_cfg = LLMConfig(
            base_url=self._cfg.api_base,
            api_key_env=self._cfg.api_key_env,
            model_name=self._cfg.model_name,
            proxy=self._cfg.proxy,
            timeout=self._cfg.timeout_sec,
            max_retries=self._cfg.max_retries,
        )
        llm_service = LLMService(llm_cfg, semaphore)
        sentiment = SentimentLLMExtractor(llm_service)

        # –¢–û–õ–¨–ö–û Regex —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
        ticker_strategy = RegexTickerStrategy()
        tickers = TickerExtractor(strategy=ticker_strategy)

        return SmartModel(
            sentiment=sentiment,
            tickers=tickers,
            cfg=self._cfg,
        )


# =========================
# üîß –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# =========================
def main():
    cfg = ModelConfig(
        proxy="http://localhost:12334",
        max_concurrency=15,
        stall_timeout_sec=60,
        api_key_env="API_KEY",
    )
    model = create_model(cfg)
    logging.basicConfig(level=logging.INFO)

    news = pd.read_csv('news.csv')
    news['publish_date'] = pd.to_datetime(news['publish_date'])

    candles = pd.read_csv('candles.csv')
    model.fit(news, candles)

    result_df = model.predict()
    for i in range(1, 21):
        future_close = result_df.groupby(['ticker'])['close'].shift(-i)
        result_df[f'p{i}'] = future_close / result_df['close'] - 1
    result_df = result_df.drop(
        columns=['timestamp', 'begin', 'close', 'volume', 'sentiment_daily'],
    ).groupby(['ticker']).head(1)
    print(result_df)
    result_df.to_csv('result_df.csv')


if __name__ == '__main__':
    main()
